# -*- coding: utf-8 -*-
"""Q1(ii).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LUttnQd62AhPNscXhoyhx4NWT8-lWxUs
"""

#Importing required modules
import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
import random
import sys
import cmath

#Extracting data from given csv file
data = pd.read_csv('A2Q1.csv', header=None, index_col=False)
X = np.array(data)

#Function that performs K-means Clustering and returns clusters and their mean after convergence of Lloyd's algorithm 
def return_len(K,mX1,mX2,mX3,mX4):
  check = 1

  it = 0
  iter = []
  errorSum = []

  #Iterations for reassignment of means to data points
  while(check):

      check = 0

      for i in range(len(K)):
          z = K[i][50]
          mindist = sys.maxsize

          #Finding minimum distance of every point to the means and assigning it that cluster

          sum1 = 0

          if(len(mX1)!=0 ):
            for q in range(50):
              sum1 = sum1 + (K[i][q]-mX1[q])**2
            if (sum1)**0.5 < mindist:
                mindist = (sum1)**0.5
                z = 1
          
          sum2 = 0

          if(len(mX2)!=0 ):
            for q in range(50):
              sum2 = sum2 + (K[i][q]-mX2[q])**2
            if (sum2)**0.5 < mindist:
                mindist = (sum2)**0.5
                z = 2
          
          sum3 = 0

          if(len(mX3)!=0 ):
            for q in range(50):
              sum3 = sum3 + (K[i][q]-mX3[q])**2
            if (sum3)**0.5 < mindist:
                mindist = (sum3)**0.5
                z = 3

          sum4 = 0
          if(len(mX4)!=0 ):
            for q in range(50):
              sum4 = sum4 + (K[i][q]-mX4[q])**2
            if (sum4)**0.5 < mindist:
                mindist = (sum4)**0.5
                z = 4
              
          if(K[i][50]!=z):
              K[i][50]=z
              check = 1

      #Storing all the data points in their respective clusters represented as lists

      X1 =[]
      X2 =[]
      X3 =[]
      X4 =[]

      for i in range(len(K)):
          
          if(K[i][50]==1):
            X11=[]
            for r in range(50):
              X11.append(K[i][r])
            X1.append(X11)

      for i in range(len(K)):
          if(K[i][50]==2):
            X12=[]
            for r in range(50):
              X12.append(K[i][r])
            X2.append(X12)

      for i in range(len(K)):
          if(K[i][50]==3):
            X13=[]
            for r in range(50):
              X13.append(K[i][r])
            X3.append(X13)

      for i in range(len(K)):
          if(K[i][50]==4):
            X14=[]
            for r in range(50):
              X14.append(K[i][r])
            X4.append(X14)

      
      #Recalculating means of all clusters after resassigment step

      mX1 = [float(sum(l))/len(l) for l in zip(*X1)]
      mX2 = [float(sum(l))/len(l) for l in zip(*X2)]
      mX3 = [float(sum(l))/len(l) for l in zip(*X3)]
      mX4 = [float(sum(l))/len(l) for l in zip(*X4)]

      it+=1
  return len(X1),len(X2),len(X3),len(X4),mX1,mX2,mX3,mX4,X1,X2,X3,X4

#Function to calculate the values of multivariate gaussian distributions by passing data point, and the mean and covariance 
def calc_multivariate_gd(x,meu,sigma):

  sigmainv = np.linalg.pinv(sigma)
  mats = np.matmul(np.matmul((x-meu),sigmainv),(np.array(x-meu).T))
  val = (-0.5) * mats
  expval = np.exp(val)

  sigmadetv = []
  eval,evec = np.linalg.eig(sigma)

  for i in range(len(eval)):
    if(eval[i]>1e-5):
      sigmadetv.append(eval[i])
  
  sigmadet = np.product(sigmadetv)

  mux = 1/np.sqrt(((2*np.pi)**len(X[0]))*sigmadet)

  finalans = (mux * expval).real

  return finalans

#Function to calculate lambda values
def calcLambda(x,meu,sigma,pi,k):
  num = pi[k] * calc_multivariate_gd(x,meu[k],sigma[k])
  den = 0

  for j in range(4):
    den += pi[j] * calc_multivariate_gd(x,meu[j],sigma[j])
  
  return num/den

#Function to calculate Pi values
def calc_pi(Lambdaik,k,n):
  num = 0

  for j in range(n):
    num += Lambdaik[j][k]

  den = n

  return num/den

#Function to calculate mean (meu) values
def calc_meu(Lambdaik,k,n):
  num = [0]*50
  den = 0

  for i in range(len(X)):
    num = np.add(num, Lambdaik[i][k] * X[i])
    den += Lambdaik[i][k]
  
  return num/den

#Function to calculate covariance (sigma) values
def calc_sigma(Lambdaik , meu,k):
  num = [[0]*50]*50

  den = 0

  for i in range(len(X)):

    x = np.array(X[i]-meu[k])
    xt = np.zeros((1,50))

    for i in range(50):
      xt[0][i] = x[i]

    num = np.add(num,Lambdaik[i][k] * np.matmul(xt,x))
    den += Lambdaik[i][k]

  return num/den

#Function to calculate Log Likelihood
def calcLL(pi,meu,sigma,n):

  logLikelihood = 0

  for i in range(n):
    ins = 0

    for k in range(4):
      ins += pi[k] * calc_multivariate_gd(X[i],meu[k],sigma[k])
    
    logLikelihood += np.log(ins)

  return logLikelihood

#Function to calculate initial covariances (sigmas)
def calc_sigma_init(cluster):
  return ((1/len(cluster)) * np.matmul(cluster.transpose(),cluster))

logL = []

#This function is taking a lot of time so instead of 100 only 5 random assignments I have done 
#5 random assignments

for i in range(5):
  w,h=len(X[0])+1,len(X)

  K=[[0 for i in range(w)] for j in range(h)]

  for i in range(len(X)):
      for j in range(len(X[0])):
          K[i][j] = X[i][j]

  for i in range(len(X)):
      K[i][50] = 0

  #Randomly choosing mean as any data point from dataset

  mX1=[]
  mX2=[]
  mX3=[]
  mX4=[]

  i1 = random.randint(0,len(K)-1)
  for i in range(50):
    mX1.append(K[i1][i])
  i2 = random.randint(0,len(K)-1)
  for i in range(50):
    mX2.append(K[i2][i])
  i3 = random.randint(0,len(K)-1)
  for i in range(50):
    mX3.append(K[i3][i])
  i4 = random.randint(0,len(K)-1)
  for i in range(50):
    mX4.append(K[i4][i])

  #Passing these random means to the Lloyd's K-means Clustering Algorithm function

  l1,l2,l3,l4,m1,m2,m3,m4,x1,x2,x3,x4 = return_len(K,mX1,mX2,mX3,mX4)

  #Calculating initial values of pi

  pi1 = l1/len(X)
  pi2 = l2/len(X)
  pi3 = l3/len(X)
  pi4 = l4/len(X)

  x1 = np.array(x1)
  x2 = np.array(x2)
  x3 = np.array(x3)
  x4 = np.array(x4)

  #Calculating initial values of mean (meu)

  m1 = np.array(m1)
  m2 = np.array(m2)
  m3 = np.array(m3)
  m4 = np.array(m4)

  x1 = x1 - m1
  x2 = x2 - m2
  x3 = x3 - m3
  x4 = x4 - m4

  #Calculating initial values of covariance (sigma)

  sigma1 = calc_sigma_init(x1)
  sigma2 = calc_sigma_init(x2)
  sigma3 = calc_sigma_init(x3)
  sigma4 = calc_sigma_init(x4)

  meul = [m1,m2,m3,m4]
  sigmal = [sigma1, sigma2,sigma3,sigma4]
  pil = [pi1,pi2,pi3,pi4]

  meuold = meul
  sigmaold = sigmal
  piold = pil;
  Lambdaikinit = []
  Loglikelihood = []

  #Calculating initial loglikelihood from initial mean and initial covariance 

  Loglikelihood.append(calcLL(piold,meuold,sigmaold,400)*10)

  #Calculating initial values of Lambda from initial mean and initial covariance

  for i in range(len(X)):
    row = []
    for j in range(4):
      val = calcLambda(X[i],meuold,sigmaold,piold,j)
      row.append(val)
    Lambdaikinit.append(row)

  #Calculating new values of pi from initial values of Lambda

  pinew = []

  for i in range(4):
    pinew.append(calc_pi(Lambdaikinit,i,len(X)))

  #Calculating new values of mean from initial values of Lambda

  meunew = []
  for i in range(4):
    meunew.append(calc_meu(Lambdaikinit,i,len(X)))

  #Calculating new values of covariances from initial values of Lambda

  sigmanew = []
  for i in range(4):
    sigmanew.append(calc_sigma(Lambdaikinit,meunew,i))

  it = 0

  #Running the EM algorithm until convergence

  while(True):
    it+=1
    
    pinew = np.array(pinew)
    piold = np.array(piold)
    meunew = np.array(meunew)
    meuold = np.array(meuold)
    sigmanew = np.array(sigmanew)
    sigmaold = np.array(sigmaold)

    #Convergence condition of EM Algorithm

    f  = []

    for i in range(4):
      diff = meunew[i]-meuold[i]
      f.append(diff)
    f = np.array(f)
    f = f.flatten()
    f_ = np.abs(np.sum(f))

    g = []

    for j in range(4):
      diff2 = sigmanew[j]-sigmaold[j]
      g.append(diff2)
    g = np.array(g)
    g = g.flatten()
    g_ = np.abs(np.sum(g))

    h_ = np.abs(np.sum(pinew-piold))

    #Algorithm runs until the new values of means and old values of means and new values of covariances and old values of covariances and new values of pi and old values of pi are almost the same
    #These new and old values are compared at a tolerance of 1e-7

    if f_<=1e-9 and g_<=1e-9 and h_<=1e-9 :
      break;

    piold = pinew
    meuold = meunew
    sigmaold = sigmanew

    #Recalculating Lambda from new values of mean,covariance and pi 

    Lambdaiknew = []

    for i in range(len(X)):
      row = []
      for j in range(4):
        x = X[i]
        val = calcLambda(x,meuold,sigmaold,piold,j)
        row.append(val)
      Lambdaiknew.append(row)

    #Recalculating pi from new Lambda

    pinew = []

    for i in range(4):
      pinew.append(calc_pi(Lambdaiknew,i,len(X)))

    #Recalculating means from new Lambda

    meunew = []
    for i in range(4):
      meunew.append(calc_meu(Lambdaiknew,i,len(X)))

    #Recalculating covariance from new Lambda

    sigmanew = []
    for i in range(4):
      sigmanew.append(calc_sigma(Lambdaiknew,meunew,i))

    #Calculating log likelihood from new values of means,pi and covariance 

    Loglikelihood.append(calcLL(pinew,meunew,sigmanew,400))

  logL.append(Loglikelihood)

#Averaging all the log likelihoods obtained from 5 random initializations(since running 100 iterations is taking too long)
ListLen = []
for lists in logL:
  ListLen.append(len(lists))

mx = max(ListLen)

clist = logL

for li in clist:
  if(len(li)<mx):
    mxele = li[len(li)-1]
    for t in range(mx-len(li)):
      li.append(mxele)

finalans=[]

finalans = np.sum(logL,axis = 0)
finalans = finalans/5

iter = []

for i_ in range(len(finalans)):
  iter.append(i_)

#Plotting average of all the log likelihoods obtained from 5 random initializations(since running 100 iterations is taking too long) as a function of iterations
g = plt.figure(1)
plt.xlabel('x-axis')
plt.ylabel('y-axis')
plt.title("Log Likelihood vs Iterations",color='black')
plt.grid()
plt.plot(iter,finalans)

plt.show()