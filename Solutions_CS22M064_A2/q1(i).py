# -*- coding: utf-8 -*-
"""Q1(i).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pvtJxAwjbtjh-kWVPWcNbIGyFY7PznT5
"""

#Importing required modules
import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
import random
import sys

#Extracting data from given csv file
data = pd.read_csv('A2Q1.csv', header=None, index_col=False)
X = np.array(data)

#Function that performs K-means Clustering and returns clusters and their mean after convergence of Lloyd's algorithm 
def return_len(K,mX1,mX2,mX3,mX4):
  check = 1

  it = 0
  iter = []
  errorSum = []

  #Iterations for reassignment of means to data points
  while(check):

      check = 0

      for i in range(len(K)):
          z = K[i][50]
          mindist = sys.maxsize

          #Finding minimum distance of every point to the means and assigning it that cluster

          sum1 = 0

          if(len(mX1)!=0 ):
            for q in range(50):
              sum1 = sum1 + (K[i][q]-mX1[q])**2
            if (sum1)**0.5 < mindist:
                mindist = (sum1)**0.5
                z = 1
          
          sum2 = 0

          if(len(mX2)!=0 ):
            for q in range(50):
              sum2 = sum2 + (K[i][q]-mX2[q])**2
            if (sum2)**0.5 < mindist:
                mindist = (sum2)**0.5
                z = 2
          
          sum3 = 0

          if(len(mX3)!=0 ):
            for q in range(50):
              sum3 = sum3 + (K[i][q]-mX3[q])**2
            if (sum3)**0.5 < mindist:
                mindist = (sum3)**0.5
                z = 3

          sum4 = 0
          if(len(mX4)!=0 ):
            for q in range(50):
              sum4 = sum4 + (K[i][q]-mX4[q])**2
            if (sum4)**0.5 < mindist:
                mindist = (sum4)**0.5
                z = 4
              
          if(K[i][50]!=z):
              K[i][50]=z
              check = 1
      
      #Storing all the data points in their respective clusters represented as lists

      X1 =[]
      X2 =[]
      X3 =[]
      X4 =[]

      for i in range(len(K)):
          
          if(K[i][50]==1):
            X11=[]
            for r in range(50):
              X11.append(K[i][r])
            X1.append(X11)

      for i in range(len(K)):
          if(K[i][50]==2):
            X12=[]
            for r in range(50):
              X12.append(K[i][r])
            X2.append(X12)

      for i in range(len(K)):
          if(K[i][50]==3):
            X13=[]
            for r in range(50):
              X13.append(K[i][r])
            X3.append(X13)

      for i in range(len(K)):
          if(K[i][50]==4):
            X14=[]
            for r in range(50):
              X14.append(K[i][r])
            X4.append(X14)

      #Recalculating means of all clusters after resassigment step

      mX1 = [float(sum(l))/len(l) for l in zip(*X1)]
      mX2 = [float(sum(l))/len(l) for l in zip(*X2)]
      mX3 = [float(sum(l))/len(l) for l in zip(*X3)]
      mX4 = [float(sum(l))/len(l) for l in zip(*X4)]

      it+=1
  return len(X1),len(X2),len(X3),len(X4),mX1,mX2,mX3,mX4

#Function to calculate lambda values
def calcLambda(x,p,pi,k):
  countx = 0
  for i in range(len(x)):
    if x[i]==1:
      countx+=1
  
  den = 0

  for j in range(4):
    den += (pi[j] * (p[j]**countx) * ((1-p[j])**(len(X[0])-countx)))  

  num = (pi[k] * (p[k]**countx) * ((1-p[k])**(len(X[0])-countx)))
  lambdaik = num/den 
  return lambdaik

#Function to calculate Pi values
def calcPi(Lambdaik,k,n):
  pi = 0

  for i in range(n):
    pi += Lambdaik[i][k]
  pi = pi/n

  return pi

#Function to calculate P values
def calcP(Lambdaik,k,n):
  num = 0
  den = 0
  for j in range(n):
    x = X[j]
    countx = 0
    for i in range(len(x)):
      if x[i]==1:
        countx+=1
    countx = countx / len(X[0])
    num += Lambdaik[j][k] * countx
    den += Lambdaik[j][k]

  if(den == 0):
    z=0
  else:
    z = num/den
  return z

#Function to calculate Log Likelihood
def calcLL(pi,p,n):

  logLikelihood = 0

  for i in range(n):
    x = X[i]
    countx = 0
    for i in range(len(x)):
      if x[i]==1:
        countx+=1
    
    ins = 0

    for k in range(4):
      ins += (pi[k]*(p[k]**countx)*((1-p[k])**(len(x)-countx)))
    
    logLikelihood += np.log(ins)

  return logLikelihood

logL = []

#100 random assignments

for i in range(100):

  w,h=len(X[0])+1,len(X)

  K=[[0 for i in range(w)] for j in range(h)]

  for i in range(len(X)):
      for j in range(len(X[0])):
          K[i][j] = X[i][j]

  for i in range(len(X)):
      K[i][50] = 0

  #Randomly choosing mean as any data point from dataset

  mX1=[]
  mX2=[]
  mX3=[]
  mX4=[]

  i1 = random.randint(0,len(K)-1)
  for i in range(50):
    mX1.append(K[i1][i])
  i2 = random.randint(0,len(K)-1)
  for i in range(50):
    mX2.append(K[i2][i])
  i3 = random.randint(0,len(K)-1)
  for i in range(50):
    mX3.append(K[i3][i])
  i4 = random.randint(0,len(K)-1)
  for i in range(50):
    mX4.append(K[i4][i])

  #Passing these random means to the Lloyd's K-means Clustering Algorithm function

  l1,l2,l3,l4,m1,m2,m3,m4 = return_len(K,mX1,mX2,mX3,mX4)

  #Calculating initial values of pi

  pi1 = l1/len(X)
  pi2 = l2/len(X)
  pi3 = l3/len(X)
  pi4 = l4/len(X)

  #Calculating initial values of p

  p1 = 0
  p2 = 0
  p3 = 0
  p4 = 0

  for i in range(50):
    if(len(m1)!=0):
      p1+=m1[i]
    if(len(m2)!=0):
      p2+=m2[i]
    if(len(m3)!=0):
      p3+=m3[i]
    if(len(m4)!=0):
      p4+=m4[i]

  p1 = p1/len(X[0])
  p2 = p2/len(X[0])
  p3 = p3/len(X[0])
  p4 = p4/len(X[0])

  pl = [p1,p2,p3,p4]
  pil = [pi1,pi2,pi3,pi4]
  pold = pl;
  piold = pil;
  
  #Calculating initial loglikelihood from pi initial and p initial 

  Loglikelihood = []

  Loglikelihood.append(calcLL(piold,pold,400))

  #Calculating initial values of Lambda from pi initial and p initial

  Lambdaikinit = []

  for i in range(len(X)):
    row = []
    for j in range(4):
      x = X[i]
      val = calcLambda(x,pold,piold,j)
      row.append(val)
    Lambdaikinit.append(row)

  #Calculating new values of pi from initial values of Lambda

  pinew = []

  for i in range(4):
    pinew.append(calcPi(Lambdaikinit,i,len(X)))

  #Calculating new values of p from initial values of Lambda

  pnew = []
  for i in range(4):
    pnew.append(calcP(Lambdaikinit,i,len(X)))

  it = 0

  #Running the EM algorithm until convergence

  while(True):
    it+=1



    pinew = np.array(pinew)
    piold = np.array(piold)
    pnew = np.array(pnew)
    pold = np.array(pold)

    #Convergence condition of EM Algorithm
    #Algorithm runs until the new values of pi and old values of pi and new values of p and old values of p are almost the same
    #These new and old values are compared at a tolerance of 1e-7 

    if (np.abs(np.sum(pinew-piold))<=1e-7) and (np.abs(np.sum(pnew-pold))<=1e-7):
      break;

    piold = pinew
    pold = pnew

    #Recalculating Lambda from new values of pi and p

    Lambdaiknew = []

    for i in range(len(X)):
      row = []
      for j in range(4):
        x = X[i]
        val = calcLambda(x,pold,piold,j)
        row.append(val)
      Lambdaiknew.append(row)

    #Recalculating pi from new Lambda

    pinew = []

    for i in range(4):
      pinew.append(calcPi(Lambdaiknew,i,len(X)))

    #Recalculating p from new Lambda

    pnew = []
    for i in range(4):
      pnew.append(calcP(Lambdaiknew,i,len(X)))

    #Calculating log likelihood from new values of p and pi

    Loglikelihood.append(calcLL(pinew,pnew,400))
  logL.append(Loglikelihood)

#Averaging all the log likelihoods obtained from 100 random initializations
ListLen = []
for lists in logL:
  ListLen.append(len(lists))

mx = max(ListLen)

clist = logL

for li in clist:
  if(len(li)<mx):
    mxele = li[len(li)-1]
    for t in range(mx-len(li)):
      li.append(mxele)

finalans=[]

finalans = np.sum(logL,axis = 0)
finalans = finalans/100

iter = []

for i_ in range(len(finalans)):
  iter.append(i_)

#Plotting average of all the log likelihoods obtained from 100 random initializations as a function of iterations
g = plt.figure(1)
plt.xlabel('x-axis')
plt.ylabel('y-axis')
plt.title("Log Likelihood vs Iterations",color='black')
plt.grid()
plt.plot(iter,finalans)

plt.show()