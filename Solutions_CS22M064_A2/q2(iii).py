# -*- coding: utf-8 -*-
"""Q2(iii).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mkd0EB6JZ_CJXGFrm39tFeBgVtVNUGci
"""

#Importing required modules
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sys
import random

#Extracting data from the given csv files
train_data = pd.read_csv('A2Q2Data_train.csv', header=None, index_col=False)
test_data = pd.read_csv('A2Q2Data_test.csv', header=None, index_col=False)
a = np.array(train_data)
t = np.array(test_data)

#Extracting all X values from training dataset
rows,cols = (len(a),(len(a[0])-1))

X = []
for i in range(rows):
  col = []
  for j in range(cols):
    col.append(0)
  X.append(col)

#Extracting all y values from training dataset
rowsy,colsy = (len(a),1)
y = []
for i in range(rowsy):
  coly = []
  for j in range(colsy):
    coly.append(0)
  y.append(coly)

for i in range(len(X)):
  for j in range(len(X[0])):
    X[i][j] = a[i][j]

for i in range(len(a)):
  y[i][0] = a[i][100]

y = np.array(y)

#Finding y transpose

yt = y.transpose()

#Converting X to array from list of lists

X = np.array(X)

#Finding X transpose

Xt = X.transpose()

#Applying ((XtX)^-1)(Xty) to find w^ml

C = np.dot(Xt,X)

Ci = np.linalg.inv(C)

#w^ml calculated

wml = np.dot(np.dot(Ci,Xt),y)

wt = []
for i in range(len(wml)):
  wt.append(1)

er = []
ite = []

#Function to apply gradient descent
def GD(Z,Zt,Zy,wt):
  it = 1
  wt = np.array(wt)
  wt = wt.reshape((len(wt),1))

  while(it<=9000):
    step = 0.001
    yp = np.dot(Z,wt)
    P = np.dot(Zt,np.subtract(yp,Zy))/100
    wt = np.subtract(wt,step*P)

    er.append(np.linalg.norm(wt-wml,2))
    ite.append(it)

    it+=1
  return wt

#Function to apply stochastic gradient descent
def SGD():
  batch_size = 100

  Z = []
  for i in range(batch_size):
    colz = []
    for j in range(batch_size):
      colz.append(0)
    Z.append(col)

  Zy = []
  for i in range(batch_size):
    colzy = []
    for j in range(1):
      colzy.append(0)
    Zy.append(coly)

  for i in range(100):
    v = np.random.randint(10000)
    Z[i] = X[v]
    Zy[i] = y[v]

  Z = np.array(Z)
  Zt = Z.transpose()
  Zy = np.array(Zy)

  wn = GD(Z,Zt,Zy,wt)
  
  return wn

#Appending all converged wt's from stochastic gradient descent
W = []
iter = []

for i in range(25):
  iter.append(i)
  wtnew = SGD()
  W.append(wtnew)
  wt = wtnew

#Calculating error of the wt's obtained from stochastic gradient descent 
error = []
for t in range(25):
  error.append(np.linalg.norm(W[t]-wml,2))

#Plotting || wt - w^ml ||2 vs t
g = plt.figure(1)
plt.xlabel('x-axis - t')
plt.ylabel('y-axis - || wt - w^ml ||2')
plt.title("|| wt - w^ml ||2 vs t",color='black')
plt.grid()
plt.plot(iter,error)

plt.show()